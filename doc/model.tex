\documentclass[11pt]{article}

\usepackage{fullpage}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{amsmath}

\title{Alignment-free relative abundance with $K$-mers}
\author{Bob Carpenter}
\date{\today}

\usepackage[colorinlistoftodos,bordercolor=orange,backgroundcolor=orange!20,linecolor=orange,textsize=scriptsize]{todonotes}
\newcommand{\rob}[1]{\todo[inline]{\textbf{Robert: }#1}}
\newcommand{\guillaume}[1]{\todo[inline]{\textbf{Guillaume: }#1}}
\newcommand{\aaron}[1]{\todo[inline]{\textbf{Felix: }#1}}

\begin{document}

\maketitle

\abstract{\noindent Given RNA-seq data and a reference transcriptome,
  we provide a statistical model of relative abundance of transcripts
  without the need to align the reads to the transcriptome by reducing
  the reads and reference transcripts to $K$-mers.}


\begin{figure}
\centering
\includegraphics[width=6cm]{transcipt-picture}
\caption{Example of taking a transciptone, sampling a read, and breaking into $4$-mers and the resulting frequency matrix}
\label{fig:transcipt-picture}
\end{figure}


\rob{Questions}
\begin{enumerate}
\item What will be our baseline of how ``well'' our method works? Compare it to the alignment method? Is there some convenient python code for this?
\item  To compare the two approaches of using simplex constraint or softmax, how can we determine which one works best? 
\end{enumerate}
\section{Introduction}

\rob{I'm confusing terminology around transcriptome, sequence isoforms and reads. Do we have multiple transciptomes or one transciptome? By reference transciptome, is that the same thing as the isoforms?  }
Our goal is to develop a fast and principled approach to determining the proportion of transciptomes, from a set of reference transciptomes, that are present in collected RNA-seq data. This in turn can be used to infer what genes are being expressed by the sampled RNA-seq. This task is normally accomplished by aligning reads from the RNA-seq data to the reference transciptomes. But this alignment process is computationally intensive and has other issues? 

Here we explore an approach that completely bypasses the need for aligning reads to the transciptome. Instead, we will take the reads from our RNA-seq data and break them into short $K$-mers, that is sequences of length $K$ where $K$ is of the order of $10$ to $15.$ See Figure~\ref{fig:transcipt-picture} for an illustration on a single read. 
Using a model of the process that transforms reads into $K$-mers, and the set of resulting $K$-mers, we are able to determine what is the most \emph{likely} set of reads and thus transciptomes that generated this set of $K$-mers.  

To give some more detail, let
 $$p-\mbox{simplex} := \{  x \in \mathbb{R}^p \; : \; \sum_{j=1}^p x_j =1, \mbox{ and }  x_i \geq 0, \quad \mbox{for } i=1,\ldots, p\}$$
 be the $p$-dimensional simplex. Assume we have a set of $G \in \mathbb{N}$ reference transciptomes and a positive vector  $\theta \in G\mbox{--simplex}$ where $\theta_j$  encodes the proportion of samples of the $j$th transciptome present in our data with respect to the total number of transciptomes. The transciptomes are then shredded into $K$-mers, of which there are a total of $4^K$ possible $K$-mers since there are four bases for DNA $(A, G, C, T)$. As such we can assign a number between $1$ and $4^K$ for all possible $K$-mers. 
Based on the process which shreds the transciptomes into $K$-mers, we will have a  matrix $X \in \mathbb{R}^{4^K \times G}$ that models the probability of getting a certain $K$-mer  by shredding a transciptome, that is
\[ X_{k, g}  \; = \; p (\mbox{ $k$th K-mer} \; \mid \; \mbox{$g$th transciptome }).\]
\rob{Something wrong, it can't be ratio of $K$-mers, and should instead be the expected number of the $k$th $K$-mer? Or the count of $K$-mers?}
We can then use this \emph{transciptome matrix} to map the ratio of transciptomes to a ratio of $k$-mers, that is for  $\theta \in \Delta^G$ we have
\[   X :\theta \mapsto X\theta \in K\mbox{--simplex}.\] 

 This establishes the \emph{forward model}, that takes the proportion of transciptomes and returns the the proportion of $K$-mers. Yet what we have in practice is the proportion of $K$-mers. So instead we count the number of each $K$-mer and store this count in a vector $y \in \mathbb{N}^{4^k}$.
We then assume that  $y$ follows a multinomial distribution and then determining $\theta$ that solves the maximum loglikelihood, that is we find $\theta^*$ such that
\[\theta^* \; \in \ ; \arg\max_{\theta \in  G\mbox{-simplex}} \log(\mathbb{P}(\theta \; | \; X, y)).\]
  Thus we are able to compute the most likely proportion of transcipts by observing the count of the $K$-mers, and building a matrix $X$ that maps from proportion of transcipts to proportion of $K$-mers.


In the following section we formalize the above informal descriptions, provide numerics.....


   \rob{Can we say something like $ X \theta  \approx \frac{y}{\mbox{sum}(y)}$? }

\section{Bases and $K$-mers}

The four \emph{DNA bases} are adenine (\texttt{A}),
cytosine (\texttt{C}), guanine (\texttt{G}), and thymine (\texttt{T}).
The set of DNA bases is
\[
  \mathbb{B} = \{ \texttt{A}, \texttt{C}, \texttt{G}, \texttt{T} \}.
\]
A \emph{$K$-mer} is an $K$-tuple of bases,
\[
  x = (x_1, \ldots, x_K),
  \ \textrm{where}
  \ x_1, \ldots, x_k \in \mathbb{B}.
\]
The set of $K$-mers is
\[
  \mathbb{B}^K
  = \{ (x_1, \ldots, x_K)
      : x_1, \ldots x_K \in \mathbb{B} \}.
\]
The set of $K$-mers of all lengths is
\[
  \mathbb{B}^* = \bigcup_{k=0}^{\infty} \mathbb{B}^k.
\]

\section{The transcriptome}

A \emph{transcriptome} is an indexed multiset of $K$-mers, each of
which may be of a different length,
\[
  T = T_{1}, \ldots, T_{G},
\]
with $T_{g, 1:K_g} \in \mathbb{B}^*$ for $g =1, \ldots, G$ and $K_g$ is the length of the $g$th transcriptome.  In data structure terms, the
transcriptome $T$ is a ragged array of bases.


\section{RNA-seq data}

Simple (unpaired) \emph{RNA-seq data}, after processing, consists of an indexed set of base
sequences, each of which is called a \emph{read},
\[
  R = R_1, \ldots, R_N, \ \textrm{where} \ R_n \in \mathbb{B}^J.
\]
Although not strictly necessary, we assume for simplicity that all of
the reads in the RNA-seq data are of the same length.

With \emph{paired-end} RNA-seq data, each read consists of two short RNA
sequences read from the same transcript with an unknown gap between
them on the transcriptome, so the data may be represented as
\[
  P = P_1, \ldots, P_N, \ \textrm{where} \ P_n \in \mathbb{B}^J \times
  \mathbb{B}^J.
\]
Again, although not strictly necessary, we assume all sequences are of
the same length to simplify notation.


\section{Shredding, indexing and counting $K$-mers}



\subsection{Indexing}
Since there are $4^K$ possible $K$-mers, we can 
define a bijection from the set of $K$-mers, $\mathbb{B}^K$, to the
g numbers, $\{ 0, 1, \ldots 4^K-1 \}$,
by treating each base as a digit in base 4 and reading the
$K$-mer as a number.  Following alphabetic order, let
\[
  \texttt{A} = 0,
  \quad \texttt{C} = 1,
  \quad \texttt{G} = 2,
  \quad \texttt{T} = 3.
\]
The resulting indexing sorts the $K$-mers into lexicographic order.
For example, with $K = 2$, the sixteen 2-mers are as follows.

\begin{center}
\begin{tabular}{ll|ll|ll|ll}
  \texttt{AA} & 0 & \texttt{CA} & 4 &   \texttt{GA} & 8 & \texttt{TA} & 12
  \\
  \texttt{AC} & 1 & \texttt{CC} & 5 &   \texttt{GC} & 9 & \texttt{TC} & 13
  \\
  \texttt{AG} & 2 & \texttt{CG} & 6 &   \texttt{GG} & 10 & \texttt{TG} & 14
  \\
  \texttt{AT} & 3 & \texttt{CT} & 7 &   \texttt{GT} & 11 & \texttt{TT} & 15
\end{tabular}
\end{center}
There are roughly one thousand 5-mers, because $4^5 = 1024$.  There are
roughly one million 10-mers and one billion 15-mers.

$K$-mer counts can be tabulated and stored very efficiently by using
this numbering to index into an array.  The complete set of 10-mer
counts from an RNA-seq experiment can be efficiently indexed and
stored in 4MB of memory using 32-bit counts (8MB with 64-bit counts).
Even smaller memory footprints may be achieved by packing smaller
integer representations; this could be beneficial if it enables the
counts to be stored in cache.  Shredding into 15-mers would take 4GB
(8GB) and clearly exceed cache capacity.  Beyond 15-mers, direct
indexing becomes prohibitive on readily available hardware, and either
a hashing scheme would be required or a lossy Bloom filter in order to
store approximate counts.  The memory used by hashing will be
proportional to the number of unique $K$-mers in the RNA-seq data.

\subsection{Shredding}
Suppose $K \leq N$.  Then the $N$-mer $x_{1:N}$ can be shredded into
a sequence of $(N - K + 1)$ $K$-mers by
\[
  x_{1:N} \ \mapsto \
  x_{1:K}, \ x_{2:K+1}, \ \ldots, \ x_{N-K+1:N}.
\]
For example, shredding   \texttt{AACAC}  into $2$-mers gives
\[
  \texttt{AACAC} \ \mapsto \
  \texttt{AA}, \ \texttt{AC}, \ \texttt{CA}, \ \texttt{AC}.
\]


\subsection{Counting}
A set of $K$-mers may be summarized with a function
$\textrm{count}_K:\mathbb{B}^* \rightarrow \mathbb{B}^K \rightarrow
\mathbb{N}$, where
$\textrm{count}_K(x)(y)$ is the number of times the $K$-mer
$y$ appears in the $N$-mer $x$.  Continuing the example above,
\begin{eqnarray*}
  \textrm{count}_2(\texttt{AACAC})(\texttt{AA}) & = & 1,
  \\
  \textrm{count}_2(\texttt{AACAC})(\texttt{AC}) & = & 2,
  \\
  \textrm{count}_2(\texttt{AACAC})(\texttt{CA}) & = & 1,
\end{eqnarray*}
and all other counts are zero. The function $ \textrm{count}_2(\texttt{AACAC}) $ can also be interpreted as a sparse vector in $\mathbb{N}^{4^k}$ with only three non-zeros elements in the indices that correspond to the $2$-mers \texttt{AA}, \texttt{AC} and \texttt{CA}.

A complete set of RNA-seq data $X = X_1, X_2, \ldots, X_N$ may then be summarized by its
$K$-mer counts by summing the counts of the individual reads $x_{n, 1:J}$
\[
  \textrm{count}_K(X) = \sum_{n=1}^N \textrm{count}_K(X_n) \in \mathbb{N}^{4^k}
\]
\rob{Are these $X_i$'s different Reads? If not, what is the relation to Reads? Also, $X$ is reserved for Transciptome matrix.}
\rob{Note to remove the below? Of ``footnote'' it.}
Paired-end RNA-seq data can be shredded elementwise.  For
example, consider extracting 3-mers from a paired read.
\[
  \texttt{ATCAG} / \texttt{CGCGC}
  \ \mapsto \
  \texttt{ATC}, \texttt{TCA}, \texttt{CAG},
  \texttt{CGC}, \texttt{GCG}, \texttt{CGC}.
\]
Counts are defined for individual paired reads and a complete set of
RNA-seq data as above.
% \rob{I don't understand this Paired-end RNA-seq part or why it makes sense to mix $k$-mers of the two pairs.}




\section{Statistical model}
Suppose we have a transcriptome consisting of $G$ base sequences. Here we describe the statistical model which maps the proportion $\theta \in G$--simplex of base sequence in the transciptome to the count of $K$-mers.
%In a transcriptome consisting of $G$ base sequences, the only
%parameter is a vector $\alpha \in \mathbb{R}^G$, representing
%intercepts in a multi-logit regression.  $\theta_g$ represents the
%relative abundance of sequence $g$ in the transcriptome.
\rob{Re-write: Now we describe the statistical model which, given the proportion of }

\subsection{Parameterizing simplexes}

%A $K$-\emph{simplex} is a $K$-vector $\theta$ of non-negative values
%such that $\textrm{sum}(\theta) = 1.$ 
 A vector $\alpha
\in \mathbb{R}^K$ can be mapped to the simplex using the softmax
function,
\[
  \textrm{softmax}(\alpha)
  = \frac{\exp(\alpha)}
         {\textrm{sum}(\exp(\alpha))} \in K\mbox{--simplex},
\]
where $\exp(\alpha)$ is defined elementwise.   By construction,
$\textrm{softmax}(\alpha)$ is a simplex, because
$\textrm{softmax}(\alpha)_i > 0$ and
$\textrm{sum}(\textrm{softmax}(\alpha)) = 1$.

As a function from $K$-vectors to $K$-simplexes, softmax is many to
one, because adding a constant to each component of $\alpha$ leads to
the same result,

\[
  \textrm{softmax}(\alpha)
  = \textrm{softmax}(\alpha + \lambda \cdot \textbf{1}_K),
\]
where $\lambda \in \mathbb{R}$ and $\textbf{1}_K$ is $K$-vector
of ones. In other words,  softmax is  not an injective function.

We can however extend the softmax function into a one-to-one mapping  as follows.
The  softmax0 function defines a smooth, one-to-one mapping from
$\mathbb{R}^{K-1}$ to $K$-simplexes by
\[
  \textrm{softmax0}(\beta) = \textrm{softmax}([0 \ \, \beta]).
\]
That is, a zero element is appended to the front of the $(K-1)$-vector.
The $\textrm{softmax0}(\beta) =\theta$ function is one-to-one since we  now  $\theta_1 = (\textrm{sum}(\exp(\alpha))^{-1}$ and thus we have that
$$ \alpha_i = \ln\left(\frac{\theta_i}{\theta_0} \right), \quad \mbox{for }i=1,\ldots, K. $$.
Thus this fixed first element can be used to determine the inputs. 
%changes the interpretation of the elements from being only
%determined relative to each other to having their scale defined
%relative to the fixed first element.
\rob{Just double checking, we actually use the $ \textrm{softmax0} $ function in our code?}
\subsection{Transcriptome matrix}

A sequence $T_g$ in the transcriptome can be transformed into a
$4^K$-simplex representing the probability of observing a given
$K$-mer given a read of transcript $g$.  The entire transcriptome can
then be converted to a $(4^K \times G)$-matrix $X$, where column $g$ is the
simplex representing the probability of a $K$-mer being observed given
that the read was from transcript $T_g$.
%\rob{Example so I understand. If $T_g = AACAC$ and we consider the $2$-mers  in their lexicographic order, then count$(AACAC) = (1, 2, 0, 0, 1, 0, 0, 0, \cdots 0)$. So in this case, is $X_{:g} = \mbox{count}(AACAC)/| \mbox{count}(AACAC)| $? In other words, it's the proportion of observed $k$-mers in $T_g$? }

\rob{A comment on how $X$ will ultimately depend on the users setup? But also, we offer some models of $X$, both in theory below, and in our code at XXX}
Next we discuss the distribution of $K$-mers, that will then be used to discuss a model of the probability of observing a given $K$-mer.

\subsubsection{Uniform read location}
\rob{This section needs an intro. Here we give one way in the which the Transciptome matrix $X$ can be constructed. By note that $X$ will ultimately depend on the technology/sampling/user setup...etc}
Even if the reads are distributed uniformly along a transcript, the
distribution of $K$-mers will not be uniform because of edge effects
and because $K$-mers may appear more than once if $K$ is small.

For example, consider a transcript \texttt{ATGGCAATG} of nine bases.
If reads are size six and we select 4-mers within those reads,
the possibilities are as follows.
\begin{table}[h!]
\centering
  \begin{tabular}{l|l}
    \textit{Reads} & 4\textit{-mers}
    \\ \hline \hline
    \texttt{ATGGCA} & \texttt{ATGG, TGGC, GGCA}
    \\ \hline
    \texttt{\hspace{0.5em}TGGCAA} & \texttt{\hspace{3em}TGGC, GGCA, GCAA}
    \\ \hline
    \texttt{\hspace{1em}GGCAAT} & \texttt{\hspace{6em}GGCA, GCAA, CAAT}
    \\ \hline
    \texttt{\hspace{1.5em}GCAATG} & \texttt{\hspace{9em}GCAA, CAAT, AATG}
  \end{tabular}
  \caption{Transcipt \texttt{ATGGCAATG} broken uniformly into $6$-mer reads which are then shredded into $4$-mers.}
  \label{tab:reads}
\end{table}
\rob{At this point, showing that Reads are produced in the same way $K$-mers are, is a bit confusing. I feel it needs some introduction/explanation. Are Reads always generated this way? Is it in fact important to explain the way Reads are generated?}
The distribution of $4$-mers resulting from uniformly sampling one of the above reads
is not expected to be uniform, but rather as follows.
\begin{center}
  \begin{tabular}{r|c||r|c}
    3\textit{-mer} & \textit{Proportion} & 3\textit{-mer} & \textit{Proportion}
    \\ \hline \hline
    \texttt{ATGG} & 1/12 & \texttt{GCAA} & 3/12
    \\ \hline
    \texttt{TGGC} & 2/12 & \texttt{CAAT} & 2/12
    \\ \hline
    \texttt{GGCA} & 3/12 & \texttt{AATG} & 1/12
  \end{tabular}
\end{center}

Assuming each position in the transcript is equally likely to produce
a read of size $J$, then $X_{1:4^K, g}$ is the $4^K$-simplex defined
by composing count functions,
\[
  X_{1:4^K, \, g}
  = \frac{\textrm{count}_K\left(\textrm{count}_J(T_g)\right)}
         {\textrm{sum}\left(\textrm{count}_K\left(\textrm{count}_J(T_g)\right)\right)},
\]
\rob{Serious notation issues: $\textrm{count}_J$ is currently defined as a function over an $N$-mer, where $N \geq J.$ Furthermore, for a given $N$-mer $T$ we have that  $\textrm{count}_J(T) \in \mathbb{N}^{4^J}.$  What does $\textrm{count}_K\left(\textrm{count}_J(T_g)\right)$ mean?  }
%\rob{Here I'm confused if this is in fact the definition of $X$ or a result of assuming uniformly distributed reads?}
where the outer function $\textrm{count}_K$ is type lifted additively
over functions as in the example.  Specifically, for a sequence $s$,
and integers $K \leq J \leq \textrm{size}(s)$, the $4^K$-vector of
$K$-mer counts found in uniformly distributed reads of size $J$ is
\[
  \textrm{count}_K(\textrm{count}_J(s))
  = \sum_{j = 0}^{4^J - 1} \textrm{count}_J(s)(j) \cdot \textrm{count}_K(j)\; \in\; \mathbb{N}^{4^K}.
\]
\rob{So this gives sums together the counts of $K$-mer in $j$, pondered by the number of reads $j$ in $s$. This makes sense only if reads are produced according to Table~\ref{tab:reads}}
\rob{Is $s$ a sequence of reads? 
}
In other words, the result $\textrm{count}_K(\textrm{count}_J(s))$ is a
function from $K$-mer identifiers to their relative abundance in
uniformly distributed $J$-gram reads over the sequence $s$.  The term
$\textrm{count}_K(j)$ is a sparse vector mapping $K$-mers to their
count in the $J$-mer represented by the integer $j$.  This is then
weighted by $\textrm{count}_J(s)(j)$, which is the count of the $J$-mer $j$ in
the sequence $s$.

\subsubsection{Non-uniform read location}

\rob{Empthasize again that this is why $X$ should be given by a user, though we offer some templates.}
The probabilty of reads is non-uniform for several reasons.  Two
causes that have large impacts are hexamer (6-mer) binding and
position within the transcript.  In the first case, the probability of
reads being chosen from a transcript varies up to two orders of
mangitude or more based on the hexamer appearing in the first six
positions of the read.  Furthermore, the strength of these effects
depends on the priming protocol used.  In the second case, the
probability of a read can vary up to an order of magnitude or more
based on its relative position along the transcript.

Given the relative probabilities of reads originating at 6-mers, the
uniform transcriptome matrix may be adjusted by reweighting based on
the probability of the 6-mers.

Given a formula for relative probability along the transcript, the
transcriptome matrix may be further adjusted for positional effects.
In the end, the result is still a $(4^K \times G)$ transcriptome
matrix $X$.

Taken together, suppose we have transcript $T_g$ of size $N$, reads
of size $J$, and $p(n)$ is the probability of a read of size $J$
originating from position $n$.



\subsection{Observed data}

The observation $y$ is a sparse $4^K$-vector of $K$-mer counts
corresponding to the shredded reads. That is

$$ y =    \textrm{count}_K(\textrm{count}_J(s)) \; \in \; \mathbb{N}^{4^k}. $$
This can be computed using....some technology thing ....
%This, together with 

\subsection{Transformed transcriptome matrix}
The transcriptome is representing as a $4^K \times G$ matrix $X$ as
described above.  The parameter $\alpha \in \mathbb{R}^G$ is a
$G$-vector whose values indicate the relative level of expression of
each transcript on the log scale; $\theta = \textrm{softmax}(\alpha)$
is the corresponding $G$-simplex representing the relative abundance
$\theta_g$ of each transcript $T_g$ in the transcriptome.

The key step is the transform of the transcript abundances,
represented by $\theta = \textrm{softmax}(\alpha)$, to $K$-mer
abundances, as represented by
$\phi = X \cdot \textrm{softmax}(\alpha)$.  Because
$\textrm{softmax}(\alpha)$ is a $G$-simplex and the columns of $X$ are
$4^K$-simplexes, the product $X \cdot \textrm{softmax}(\alpha)$ is a
$4^K$ simplex---it just reweights the gene-level $K$-mer expressions
represented by the columns by the gene expressions represented by
$\textrm{softmax}(\alpha)$.
\rob{This section should be moved and merged with section 6.2, or just removed. }

\subsection{Sampling distribution}

Since $y$ is a vector of counts of $K$-mers, we make the natural assumption that $y$ was generated 
by a multinomial distribution with
\[
  y \sim \textrm{multinomial}(X \cdot \textrm{softmax}(\alpha)).
\]
Here we have used  $X \cdot \textrm{softmax}(\alpha)$, that is the proportion of observed $K$-mers, as the probabilities of sampling a $K$-mer.

%
%The sampling distribution $p(y \mid X, \alpha)$  for the observed
%$K$-mer counts $y$ given the transcriptome matrix $X$ and log odds
%expression simplex $\theta$ is defined to be multinomial,



\subsection{Prior}

The prior is a normal centered at the origin,
\[
  \alpha_g \sim \textrm{normal}(0, \lambda),
\]
for some scale $\lambda > 0$.  The smaller the scale $\lambda$, the
more parameter estimates for $\alpha_g$ are shrunk toward zero, and
thus the less relative difference in expression is allowed.

If we intend to get true zero estimates for some genes with a
continuous prior, we will need to make penalized maximum likelihood
estimates with an L1-type penalty.  Keeping to sampling notation, this
is represented by a double-exponential exponential (aka Laplace)
distribution,
\[
  \alpha_g \sim \textrm{double-exponential}(0, \lambda).
\]

\subsection{Posterior}

Given our statistical model, our goal is now to find the most likely $\alpha \in \mathbb{R}^{G}$ given $y$ and $X$. That is, we now want to solve
\begin{equation}
\alpha^* \; \in \; \arg \max \log p(\alpha \; \mid \; X,y).
\end{equation}

Fortunately, this posterior distribution has a simple closed form that lends itself to optimization.
 Indeed, by Bayes's rule up to an additive constant
that does not depend on the parameters $\alpha$ we have that
\begin{align}\label{eq:posterior}
  \log p(\alpha \mid X, y)& = \log p(y \mid X, \alpha) + \log p(\alpha) +
  \textrm{const.}  \\
  &= y^{\top} \cdot \log \left( X^{\top} \cdot \textrm{softmax}(\alpha) \right)
          - \frac{1}{2 \cdot \lambda^2} \cdot \alpha^{\top} \cdot \alpha +\textrm{const.} \nonumber
\end{align} 
 


%\subsubsection{Posterior gradient}
We can also efficiently compute the gradient\footnote{The derivative is easy to work out
  by passing \url{matrixcalculus.org} the query
  \begin{center}\texttt{y' * log
(X * exp(a) / sum(exp(a))) - a' * a / (2 * lambda)}\end{center}
and then simplifying using $\textrm{softmax}$.} of posterior using 
%Efficient maximum penalized likelihood estimation and full Bayesian inference
%require evaluating gradients of the posterior with respect to the parameters
%$\alpha$.  The gradient is
%%
\[
  \begin{array}{l}
  \nabla\!_{\alpha} \, \log p(\alpha \mid X, y) + \log p(\alpha)
  \\[4pt]
  \qquad = \
    t \odot \textrm{softmax}(\alpha)
    - \textrm{softmax}(\alpha)^{\top}\! \cdot t \cdot \textrm{softmax}(\alpha)
    - \frac{\displaystyle \alpha}{\displaystyle \lambda},
\end{array}
\]
where
\[
  t = X^{\top}\! \cdot (y \oslash (X \cdot \textrm{softmax}(\alpha)))
\]
and $\odot$ represents elementwise multiplication and $\oslash$
elementwise division. 


\section{Alternative Representation}

Instead of maximizing~\eqref{eq:posterior} using the softmax parametrization, we can instead use a constraint and solve

\begin{align}
\theta^* \; \in \; \arg \max_{\theta \in G\mbox{--simplex}}  \log p(\theta \mid X, y) &= y^{\top} \cdot \log \left( X^{\top} \cdot \theta \right)
         +\log p(\theta)+\textrm{const.} \label{eq:posterior-const}
\end{align} 
As for the prior $p(\theta)$, since $\theta$ is in the simplex, we could use entropy, that is $p(\theta) = -\sum_{i=1}^G \theta_i \log(\theta_i).$ But what does this mean in terms of probabilistic assumptions?

The advantage of~\eqref{eq:posterior-const} is that, with a strongly convex prior, the resulting optimization problem is strongly convex and smooth, and thus can be provenly solve with L-BFGS.

\section{Estimating expression}

\rob{Are all these alternative approaches to maximizing the posterior of $\alpha$?}
\subsection{Bayesian posterior mean estimate}

The Bayesian \emph{posterior mean estimate} is the parameter's
expected value conditioned on the observed data,
\begin{eqnarray*}
  \widehat{\alpha}
  & = & \mathbb{E}\!\left[\alpha \mid X, y \, \right]
  \\[6pt]
  & = & \int_{\mathbb{R}^G} \alpha \cdot p(\alpha \mid X, y) \, \textrm{d}\alpha.
\end{eqnarray*}
The posterior mean estimate minimizes expected square estimation error
for $\alpha$ if the model is well specified.

Given a sampler that can produce Markov chain Monte Carlo (MCMC) draws
distributed according to the posterior
\[
  \alpha^{(m)} \sim p(\alpha \mid X, y),
\]
the posterior mean estimate may be calculated as
\[
  \widehat{\alpha} \approx \frac{1}{M} \sum_{m = 1}^M \alpha^{(m)}.
\]
As $M \rightarrow \infty$, the approximation converges to the true
value.  Assuming the Markov chain is geometrically ergodic, the MCMC
central limit theorem applies, providing a convergence rate of
$\mathcal{O}\left(1 / \sqrt{M}\right)$.  In practice, a few hundred
effective draws brings the expected error on $\widehat{\alpha}$ due to
MCMC down to less than a tenth of the posterior standard deviation of
$\alpha$.

\subsection{Maximum a posteriori estimate}
\rob{Why are we mentioning this?}
The \emph{max a posteriori} (MAP) estimate for $\alpha$ is given by
choosing the parameter value $\alpha^*$ that maximizes the posterior
density,
\[
  \alpha^* = \textrm{arg\,max}_{\alpha} \,
  \log \textrm{multinomial}(y \mid x \cdot \textrm{softmax}(\alpha))
  + \log \textrm{normal}(\alpha \mid 0, \lambda \cdot \textrm{I}).
\]
This is not a Bayesian estimate because it does not average over
posterior uncertainty.


\subsection{Maximum penalized likelihood estimate}

A pure frequentist estimate may be defined by casting the normal prior
on the parameters as a penalty function based on the $\textrm{L}_2$
norm,
\[
  -\frac{1}{2 \cdot \lambda^2} \cdot \alpha^{\top}\!\! \cdot \alpha.
\]

Replacing the normal prior in the definition of the MAP estimate with
the $\textrm{L}_2$ penalty defined above, the
\emph{penalized maximum likelihood estimate} (MLE) is defined as
\[
  \alpha^* = \textrm{arg\,max}_{\alpha} \,
  \log \textrm{multinomial}(y \mid x \cdot \textrm{softmax}(\alpha))
  - \frac{1}{2 \cdot \lambda^2} \cdot \alpha^{\top}\!\! \cdot \alpha.
\]


\section*{References}

\begin{itemize}
\item Biases in Illumina transcriptome sequencing caused by random
  hexamer priming Kasper D. Hansen,1,* Steven E. Brenner,2 and
  Sandrine Dudoit1,3.  Nucleic acids research.
\end{itemize}

\end{document}
